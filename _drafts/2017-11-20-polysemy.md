---
layout: post
title:  "Polysemy: Words with multiple meanings"
date:   2017-11-19 00:00:00 -0800
categories: ['Deep Learning', 'Representations', 'Natural Language Processing', 'Word Vectors']
published: true
---

One of the problems with word vector models is when a word has multiple meanings. For example "change" can mean to morph, indicate the currency you are returned, or even more nuanced, mean coins. It is a lot ot ask of a single word vector to represent multiple meanings.

We can get around this by representing each word token by a set of word vectors, i.e. if their are l different word vectors per token, it would be an kxl matrix where l is the number of word vectors and k is the dimensionality of each vector.

Then when computing the dot product of a context word with the outer words, we multiply the matrices for both words rather than two single vectors.

$$Z = V_i^T U_i$$

Each element of $$Z$$, $$Z_{(c, d)}$$, is the dot product between the cth inner vector of word i and the dth outer vector of word j.

Now because we want a single best value, we take the max value over the set of dot products in the resulting matrix.

$$max(V_i^T U_i)$$

This allows the model to learn more than one vector per word. We can enforce sparsity by adding a L1 norm from the mean vector for each vector.

$$|V_i - mean(V_i)|$$
